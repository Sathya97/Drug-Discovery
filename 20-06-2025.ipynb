{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "research-overview",
   "metadata": {},
   "source": [
    "# Novel Multi-Modal Transformer Architecture for Drug Mechanism of Action Prediction\n",
    "\n",
    "## Abstract\n",
    "This notebook presents a novel approach to drug mechanism of action (MoA) prediction using a **Multi-Modal Molecular Transformer (M3T)** architecture. Unlike traditional Graph Convolutional Networks, our approach combines:\n",
    "\n",
    "1. **Molecular BERT-style Transformer** for SMILES sequence encoding\n",
    "2. **Chemical Descriptor Fusion Network** for molecular properties\n",
    "3. **Cross-Modal Attention Mechanism** for feature integration\n",
    "4. **Contrastive Learning** for improved representation learning\n",
    "\n",
    "## Key Innovations:\n",
    "- ðŸš€ **Transformer-based molecular encoding** without graph structure assumptions\n",
    "- ðŸš€ **Multi-modal fusion** of sequential and numerical molecular features\n",
    "- ðŸš€ **Contrastive pre-training** for better molecular representations\n",
    "- ðŸš€ **Attention visualization** for mechanism interpretability\n",
    "- ðŸš€ **Statistical significance testing** against GCN baseline\n",
    "\n",
    "## Research Hypothesis\n",
    "We hypothesize that treating molecular SMILES as natural language sequences and applying transformer attention mechanisms will capture long-range molecular dependencies better than local graph convolutions, leading to improved MoA prediction accuracy and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imports-setup",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rdkit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stats\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Molecular informatics\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrdkit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chem\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrdkit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mChem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Descriptors, rdMolDescriptors\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrdkit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mChem\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrdMolDescriptors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GetMorganFingerprintAsBitVect\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rdkit'"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep learning frameworks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Scientific computing\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score, \n",
    "    precision_recall_curve, classification_report,\n",
    "    multilabel_confusion_matrix\n",
    ")\n",
    "from scipy import stats\n",
    "\n",
    "# Molecular informatics\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, rdMolDescriptors\n",
    "from rdkit.Chem.rdMolDescriptors import GetMorganFingerprintAsBitVect\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \n",
    "                     \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-acquisition",
   "metadata": {},
   "source": [
    "## 1. Enhanced Data Acquisition and Preprocessing\n",
    "\n",
    "We extend the original ChEMBL query to include additional molecular and target information for multi-modal learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chembl-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to ChEMBL database\n",
    "db_path = os.path.expanduser(\"~/Downloads/chembl_35/chembl_35_sqlite/chembl_35.db\")\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Enhanced query with additional molecular and target information\n",
    "enhanced_query = \"\"\"\n",
    "SELECT DISTINCT\n",
    "    dm.molregno,\n",
    "    dm.mechanism_of_action,\n",
    "    dm.action_type,\n",
    "    cs.canonical_smiles,\n",
    "    cs.standard_inchi_key,\n",
    "    cp.full_mwt,\n",
    "    cp.alogp,\n",
    "    cp.hba,\n",
    "    cp.hbd,\n",
    "    cp.psa,\n",
    "    cp.rtb,\n",
    "    cp.ro3_pass,\n",
    "    cp.num_ro5_violations,\n",
    "    td.target_type,\n",
    "    td.organism\n",
    "FROM drug_mechanism dm\n",
    "JOIN compound_structures cs ON dm.molregno = cs.molregno\n",
    "JOIN compound_properties cp ON dm.molregno = cp.molregno\n",
    "LEFT JOIN target_dictionary td ON dm.tid = td.tid\n",
    "WHERE cs.canonical_smiles IS NOT NULL \n",
    "    AND dm.mechanism_of_action IS NOT NULL\n",
    "    AND LENGTH(cs.canonical_smiles) BETWEEN 10 AND 200\n",
    "LIMIT 15000;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Executing enhanced ChEMBL query...\")\n",
    "df_raw = pd.read_sql(enhanced_query, conn)\n",
    "print(f\"Retrieved {len(df_raw)} drug-mechanism pairs\")\n",
    "print(f\"Unique mechanisms: {df_raw['mechanism_of_action'].nunique()}\")\n",
    "print(f\"Unique molecules: {df_raw['molregno'].nunique()}\")\n",
    "\n",
    "# Display sample data\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-preprocessing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing and quality control\n",
    "def preprocess_molecular_data(df):\n",
    "    \"\"\"Enhanced preprocessing with molecular validation and feature engineering.\"\"\"\n",
    "    \n",
    "    print(\"Starting molecular data preprocessing...\")\n",
    "    \n",
    "    # Remove duplicates and invalid SMILES\n",
    "    df_clean = df.drop_duplicates(subset=['molregno', 'mechanism_of_action'])\n",
    "    \n",
    "    # Validate SMILES strings\n",
    "    valid_smiles = []\n",
    "    for smiles in tqdm(df_clean['canonical_smiles'], desc=\"Validating SMILES\"):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        valid_smiles.append(mol is not None)\n",
    "    \n",
    "    df_clean = df_clean[valid_smiles].reset_index(drop=True)\n",
    "    print(f\"Valid molecules after SMILES validation: {len(df_clean)}\")\n",
    "    \n",
    "    # Filter mechanisms with sufficient samples (min 5 examples)\n",
    "    mechanism_counts = df_clean['mechanism_of_action'].value_counts()\n",
    "    frequent_mechanisms = mechanism_counts[mechanism_counts >= 5].index\n",
    "    df_filtered = df_clean[df_clean['mechanism_of_action'].isin(frequent_mechanisms)]\n",
    "    \n",
    "    print(f\"Mechanisms with â‰¥5 examples: {len(frequent_mechanisms)}\")\n",
    "    print(f\"Final dataset size: {len(df_filtered)}\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "# Apply preprocessing\n",
    "df_processed = preprocess_molecular_data(df_raw)\n",
    "\n",
    "# Display mechanism distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "mechanism_counts = df_processed['mechanism_of_action'].value_counts().head(20)\n",
    "plt.barh(range(len(mechanism_counts)), mechanism_counts.values)\n",
    "plt.yticks(range(len(mechanism_counts)), mechanism_counts.index, fontsize=8)\n",
    "plt.xlabel('Number of Compounds')\n",
    "plt.title('Top 20 Most Frequent Mechanisms of Action')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"Total samples: {len(df_processed)}\")\n",
    "print(f\"Unique mechanisms: {df_processed['mechanism_of_action'].nunique()}\")\n",
    "print(f\"Average SMILES length: {df_processed['canonical_smiles'].str.len().mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-engineering",
   "metadata": {},
   "source": [
    "## 2. Multi-Modal Feature Engineering\n",
    "\n",
    "We create three types of molecular representations:\n",
    "1. **Sequential**: Tokenized SMILES for transformer input\n",
    "2. **Numerical**: Chemical descriptors and properties\n",
    "3. **Structural**: Morgan fingerprints for contrastive learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-features",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StandardScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 85\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(tokenized), vocab, char_to_idx\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Extract all molecular features\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m feature_extractor \u001b[38;5;241m=\u001b[39m MolecularFeatureExtractor(max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting multi-modal molecular features...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# 1. Chemical descriptors\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m, in \u001b[0;36mMolecularFeatureExtractor.__init__\u001b[0;34m(self, max_length)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m=\u001b[39m max_length\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StandardScaler' is not defined"
     ]
    }
   ],
   "source": [
    "class MolecularFeatureExtractor:\n",
    "    \"\"\"Extract multi-modal molecular features for transformer input.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_length=128):\n",
    "        self.max_length = max_length\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def extract_chemical_descriptors(self, smiles_list):\n",
    "        \"\"\"Extract comprehensive chemical descriptors using RDKit.\"\"\"\n",
    "        \n",
    "        descriptors = []\n",
    "        descriptor_names = [\n",
    "            'MolWt', 'LogP', 'NumHDonors', 'NumHAcceptors', 'TPSA',\n",
    "            'NumRotatableBonds', 'NumAromaticRings', 'NumSaturatedRings',\n",
    "            'NumAliphaticRings', 'RingCount', 'FractionCsp3',\n",
    "            'NumHeteroatoms', 'BertzCT', 'BalabanJ', 'Ipc'\n",
    "        ]\n",
    "        \n",
    "        for smiles in tqdm(smiles_list, desc=\"Extracting descriptors\"):\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                descriptors.append([0] * len(descriptor_names))\n",
    "                continue\n",
    "                \n",
    "            desc_values = [\n",
    "                Descriptors.MolWt(mol),\n",
    "                Descriptors.MolLogP(mol),\n",
    "                Descriptors.NumHDonors(mol),\n",
    "                Descriptors.NumHAcceptors(mol),\n",
    "                Descriptors.TPSA(mol),\n",
    "                Descriptors.NumRotatableBonds(mol),\n",
    "                Descriptors.NumAromaticRings(mol),\n",
    "                Descriptors.NumSaturatedRings(mol),\n",
    "                Descriptors.NumAliphaticRings(mol),\n",
    "                Descriptors.RingCount(mol),\n",
    "                Descriptors.FractionCsp3(mol),\n",
    "                Descriptors.NumHeteroatoms(mol),\n",
    "                Descriptors.BertzCT(mol),\n",
    "                Descriptors.BalabanJ(mol),\n",
    "                Descriptors.Ipc(mol)\n",
    "            ]\n",
    "            descriptors.append(desc_values)\n",
    "            \n",
    "        return np.array(descriptors), descriptor_names\n",
    "    \n",
    "    def extract_morgan_fingerprints(self, smiles_list, radius=2, n_bits=1024):\n",
    "        \"\"\"Extract Morgan fingerprints for structural similarity.\"\"\"\n",
    "        \n",
    "        fingerprints = []\n",
    "        for smiles in tqdm(smiles_list, desc=\"Extracting fingerprints\"):\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                fingerprints.append(np.zeros(n_bits))\n",
    "                continue\n",
    "                \n",
    "            fp = GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n",
    "            fingerprints.append(np.array(fp))\n",
    "            \n",
    "        return np.array(fingerprints)\n",
    "    \n",
    "    def tokenize_smiles(self, smiles_list):\n",
    "        \"\"\"Tokenize SMILES strings for transformer input.\"\"\"\n",
    "        \n",
    "        # Create character-level vocabulary from SMILES\n",
    "        all_chars = set(''.join(smiles_list))\n",
    "        vocab = ['<PAD>', '<UNK>', '<START>', '<END>'] + sorted(list(all_chars))\n",
    "        char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "        \n",
    "        tokenized = []\n",
    "        for smiles in smiles_list:\n",
    "            tokens = [char_to_idx.get(char, char_to_idx['<UNK>']) for char in smiles]\n",
    "            # Add start/end tokens and pad/truncate\n",
    "            tokens = [char_to_idx['<START>']] + tokens + [char_to_idx['<END>']]\n",
    "            \n",
    "            if len(tokens) > self.max_length:\n",
    "                tokens = tokens[:self.max_length]\n",
    "            else:\n",
    "                tokens.extend([char_to_idx['<PAD>']] * (self.max_length - len(tokens)))\n",
    "                \n",
    "            tokenized.append(tokens)\n",
    "            \n",
    "        return np.array(tokenized), vocab, char_to_idx\n",
    "\n",
    "# Extract all molecular features\n",
    "feature_extractor = MolecularFeatureExtractor(max_length=128)\n",
    "\n",
    "print(\"Extracting multi-modal molecular features...\")\n",
    "\n",
    "# 1. Chemical descriptors\n",
    "descriptors, descriptor_names = feature_extractor.extract_chemical_descriptors(\n",
    "    df_processed['canonical_smiles'].tolist()\n",
    ")\n",
    "\n",
    "# 2. Morgan fingerprints\n",
    "fingerprints = feature_extractor.extract_morgan_fingerprints(\n",
    "    df_processed['canonical_smiles'].tolist()\n",
    ")\n",
    "\n",
    "# 3. Tokenized SMILES\n",
    "tokenized_smiles, vocab, char_to_idx = feature_extractor.tokenize_smiles(\n",
    "    df_processed['canonical_smiles'].tolist()\n",
    ")\n",
    "\n",
    "print(f\"Chemical descriptors shape: {descriptors.shape}\")\n",
    "print(f\"Morgan fingerprints shape: {fingerprints.shape}\")\n",
    "print(f\"Tokenized SMILES shape: {tokenized_smiles.shape}\")\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "# Normalize chemical descriptors\n",
    "descriptors_normalized = feature_extractor.scaler.fit_transform(descriptors)\n",
    "\n",
    "# Prepare labels for multi-label classification\n",
    "mlb = MultiLabelBinarizer()\n",
    "# Convert single mechanisms to list format for MultiLabelBinarizer\n",
    "mechanism_lists = [[mech] for mech in df_processed['mechanism_of_action']]\n",
    "labels = mlb.fit_transform(mechanism_lists)\n",
    "\n",
    "print(f\"\\nLabel encoding:\")\n",
    "print(f\"Number of unique mechanisms: {len(mlb.classes_)}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "print(f\"Label sparsity: {(labels == 0).sum() / labels.size:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-architecture",
   "metadata": {},
   "source": [
    "## 3. Multi-Modal Molecular Transformer (M3T) Architecture\n",
    "\n",
    "Our novel architecture combines:\n",
    "- **SMILES Transformer Encoder**: Processes tokenized molecular sequences\n",
    "- **Chemical Descriptor Network**: Handles numerical molecular properties\n",
    "- **Cross-Modal Attention**: Fuses sequential and numerical representations\n",
    "- **Contrastive Learning Head**: Improves molecular representation quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transformer-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding for transformer input.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class SMILESTransformerEncoder(nn.Module):\n",
    "    \"\"\"Transformer encoder for SMILES sequences.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model=256, nhead=8, num_layers=6, \n",
    "                 dim_feedforward=1024, max_len=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask=None):\n",
    "        # src shape: (batch_size, seq_len)\n",
    "        src = self.embedding(src) * np.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src.transpose(0, 1)).transpose(0, 1)\n",
    "        src = self.dropout(src)\n",
    "        \n",
    "        # Create padding mask\n",
    "        if src_mask is None:\n",
    "            src_mask = (src.sum(dim=-1) == 0)  # Padding positions\n",
    "            \n",
    "        output = self.transformer_encoder(src, src_key_padding_mask=src_mask)\n",
    "        \n",
    "        # Global average pooling (excluding padding)\n",
    "        mask_expanded = (~src_mask).unsqueeze(-1).float()\n",
    "        output_masked = output * mask_expanded\n",
    "        output_pooled = output_masked.sum(dim=1) / mask_expanded.sum(dim=1)\n",
    "        \n",
    "        return output_pooled, output  # pooled representation and full sequence\n",
    "\n",
    "class ChemicalDescriptorNetwork(nn.Module):\n",
    "    \"\"\"Neural network for processing chemical descriptors.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims=[512, 256, 128], dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "            \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class CrossModalAttention(nn.Module):\n",
    "    \"\"\"Cross-modal attention mechanism for fusing different representations.\"\"\"\n",
    "    \n",
    "    def __init__(self, seq_dim, desc_dim, hidden_dim=256, num_heads=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.seq_proj = nn.Linear(seq_dim, hidden_dim)\n",
    "        self.desc_proj = nn.Linear(desc_dim, hidden_dim)\n",
    "        \n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, seq_features, desc_features):\n",
    "        # Project to same dimension\n",
    "        seq_proj = self.seq_proj(seq_features)  # (batch, hidden_dim)\n",
    "        desc_proj = self.desc_proj(desc_features)  # (batch, hidden_dim)\n",
    "        \n",
    "        # Add sequence dimension for attention\n",
    "        seq_proj = seq_proj.unsqueeze(1)  # (batch, 1, hidden_dim)\n",
    "        desc_proj = desc_proj.unsqueeze(1)  # (batch, 1, hidden_dim)\n",
    "        \n",
    "        # Cross attention: seq attends to desc\n",
    "        attn_output, attn_weights = self.multihead_attn(\n",
    "            query=seq_proj,\n",
    "            key=desc_proj,\n",
    "            value=desc_proj\n",
    "        )\n",
    "        \n",
    "        # Residual connection and normalization\n",
    "        output = self.norm1(seq_proj + attn_output)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        ffn_output = self.ffn(output)\n",
    "        output = self.norm2(output + ffn_output)\n",
    "        \n",
    "        return output.squeeze(1), attn_weights  # Remove sequence dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalMolecularTransformer(nn.Module):\n",
    "    \"\"\"Main M3T model combining all components.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, num_descriptors, num_classes, \n",
    "                 d_model=256, nhead=8, num_layers=6, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Component networks\n",
    "        self.smiles_encoder = SMILESTransformerEncoder(\n",
    "            vocab_size=vocab_size,\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.descriptor_network = ChemicalDescriptorNetwork(\n",
    "            input_dim=num_descriptors,\n",
    "            hidden_dims=[512, 256, 128],\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.cross_modal_attention = CrossModalAttention(\n",
    "            seq_dim=d_model,\n",
    "            desc_dim=128,\n",
    "            hidden_dim=256,\n",
    "            num_heads=nhead\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Contrastive learning head\n",
    "        self.contrastive_head = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "        \n",
    "    def forward(self, smiles_tokens, descriptors, return_attention=False):\n",
    "        # Encode SMILES sequences\n",
    "        seq_features, seq_full = self.smiles_encoder(smiles_tokens)\n",
    "        \n",
    "        # Process chemical descriptors\n",
    "        desc_features = self.descriptor_network(descriptors)\n",
    "        \n",
    "        # Cross-modal fusion\n",
    "        fused_features, attention_weights = self.cross_modal_attention(\n",
    "            seq_features, desc_features\n",
    "        )\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(fused_features)\n",
    "        \n",
    "        # Contrastive representation\n",
    "        contrastive_repr = self.contrastive_head(fused_features)\n",
    "        \n",
    "        if return_attention:\n",
    "            return logits, contrastive_repr, attention_weights\n",
    "        else:\n",
    "            return logits, contrastive_repr\n",
    "\n",
    "# Initialize model\n",
    "model = MultiModalMolecularTransformer(\n",
    "    vocab_size=len(vocab),\n",
    "    num_descriptors=descriptors_normalized.shape[1],\n",
    "    num_classes=labels.shape[1],\n",
    "    d_model=256,\n",
    "    nhead=8,\n",
    "    num_layers=6,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# Model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nM3T Model Architecture:\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size: {total_params * 4 / 1024**2:.1f} MB\")\n",
    "\n",
    "# Test forward pass\n",
    "with torch.no_grad():\n",
    "    sample_smiles = torch.tensor(tokenized_smiles[:4]).to(device)\n",
    "    sample_desc = torch.tensor(descriptors_normalized[:4]).float().to(device)\n",
    "    \n",
    "    logits, contrastive = model(sample_smiles, sample_desc)\n",
    "    print(f\"\\nForward pass test:\")\n",
    "    print(f\"Input SMILES shape: {sample_smiles.shape}\")\n",
    "    print(f\"Input descriptors shape: {sample_desc.shape}\")\n",
    "    print(f\"Output logits shape: {logits.shape}\")\n",
    "    print(f\"Contrastive representation shape: {contrastive.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-preparation",
   "metadata": {},
   "source": [
    "## 4. Dataset Preparation and Stratified Splitting\n",
    "\n",
    "We implement stratified splitting to ensure balanced representation of mechanisms across train/validation/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-splitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolecularDataset(Dataset):\n",
    "    \"\"\"PyTorch dataset for multi-modal molecular data.\"\"\"\n",
    "    \n",
    "    def __init__(self, smiles_tokens, descriptors, fingerprints, labels):\n",
    "        self.smiles_tokens = torch.tensor(smiles_tokens, dtype=torch.long)\n",
    "        self.descriptors = torch.tensor(descriptors, dtype=torch.float32)\n",
    "        self.fingerprints = torch.tensor(fingerprints, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.smiles_tokens)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'smiles_tokens': self.smiles_tokens[idx],\n",
    "            'descriptors': self.descriptors[idx],\n",
    "            'fingerprints': self.fingerprints[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Stratified splitting for multi-label data\n",
    "def stratified_multilabel_split(X, y, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    \"\"\"Stratified split for multi-label classification.\"\"\"\n",
    "    \n",
    "    # Create a single label for stratification (most frequent mechanism)\n",
    "    stratify_labels = y.argmax(axis=1)\n",
    "    \n",
    "    # First split: train+val vs test\n",
    "    X_temp, X_test, y_temp, y_test, strat_temp, strat_test = train_test_split(\n",
    "        X, y, stratify_labels, test_size=test_size, \n",
    "        stratify=stratify_labels, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Second split: train vs val\n",
    "    val_size_adjusted = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_size_adjusted,\n",
    "        stratify=strat_temp, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Prepare data for splitting\n",
    "X_combined = np.arange(len(tokenized_smiles))  # Just indices\n",
    "\n",
    "# Perform stratified split\n",
    "idx_train, idx_val, idx_test, y_train, y_val, y_test = stratified_multilabel_split(\n",
    "    X_combined, labels, test_size=0.2, val_size=0.1, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MolecularDataset(\n",
    "    tokenized_smiles[idx_train],\n",
    "    descriptors_normalized[idx_train],\n",
    "    fingerprints[idx_train],\n",
    "    y_train\n",
    ")\n",
    "\n",
    "val_dataset = MolecularDataset(\n",
    "    tokenized_smiles[idx_val],\n",
    "    descriptors_normalized[idx_val],\n",
    "    fingerprints[idx_val],\n",
    "    y_val\n",
    ")\n",
    "\n",
    "test_dataset = MolecularDataset(\n",
    "    tokenized_smiles[idx_test],\n",
    "    descriptors_normalized[idx_test],\n",
    "    fingerprints[idx_test],\n",
    "    y_test\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, \n",
    "    num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False,\n",
    "    num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False,\n",
    "    num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Dataset splits:\")\n",
    "print(f\"Training: {len(train_dataset)} samples\")\n",
    "print(f\"Validation: {len(val_dataset)} samples\")\n",
    "print(f\"Test: {len(test_dataset)} samples\")\n",
    "print(f\"\\nBatch configuration:\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Verify label distribution\n",
    "train_label_dist = y_train.sum(axis=0)\n",
    "val_label_dist = y_val.sum(axis=0)\n",
    "test_label_dist = y_test.sum(axis=0)\n",
    "\n",
    "print(f\"\\nLabel distribution verification:\")\n",
    "print(f\"Train labels per class (meanÂ±std): {train_label_dist.mean():.1f}Â±{train_label_dist.std():.1f}\")\n",
    "print(f\"Val labels per class (meanÂ±std): {val_label_dist.mean():.1f}Â±{val_label_dist.std():.1f}\")\n",
    "print(f\"Test labels per class (meanÂ±std): {test_label_dist.mean():.1f}Â±{test_label_dist.std():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-setup",
   "metadata": {},
   "source": [
    "## 5. Training Setup with Contrastive Learning\n",
    "\n",
    "We implement a hybrid loss function combining:\n",
    "1. **Multi-label classification loss** (Binary Cross Entropy)\n",
    "2. **Contrastive learning loss** (InfoNCE) for better molecular representations\n",
    "3. **Regularization terms** for model stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loss-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"InfoNCE contrastive loss for molecular representation learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, temperature=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def forward(self, representations, fingerprints):\n",
    "        \"\"\"Compute contrastive loss between learned representations and fingerprints.\"\"\"\n",
    "        \n",
    "        batch_size = representations.size(0)\n",
    "        \n",
    "        # Normalize representations\n",
    "        repr_norm = F.normalize(representations, dim=1)\n",
    "        fp_norm = F.normalize(fingerprints, dim=1)\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        similarity_matrix = torch.matmul(repr_norm, fp_norm.T) / self.temperature\n",
    "        \n",
    "        # Create labels (diagonal should be positive pairs)\n",
    "        labels = torch.arange(batch_size).to(representations.device)\n",
    "        \n",
    "        # Compute InfoNCE loss\n",
    "        loss = F.cross_entropy(similarity_matrix, labels)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "class HybridLoss(nn.Module):\n",
    "    \"\"\"Combined loss function for multi-task learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, classification_weight=1.0, contrastive_weight=0.1, \n",
    "                 temperature=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.classification_weight = classification_weight\n",
    "        self.contrastive_weight = contrastive_weight\n",
    "        \n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        self.contrastive_loss = ContrastiveLoss(temperature)\n",
    "        \n",
    "    def forward(self, logits, labels, representations, fingerprints):\n",
    "        # Classification loss\n",
    "        cls_loss = self.bce_loss(logits, labels)\n",
    "        \n",
    "        # Contrastive loss\n",
    "        cont_loss = self.contrastive_loss(representations, fingerprints)\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = (self.classification_weight * cls_loss + \n",
    "                     self.contrastive_weight * cont_loss)\n",
    "        \n",
    "        return total_loss, cls_loss, cont_loss\n",
    "\n",
    "# Initialize loss function and optimizer\n",
    "criterion = HybridLoss(\n",
    "    classification_weight=1.0,\n",
    "    contrastive_weight=0.1,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-5,\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    ")\n",
    "\n",
    "print(\"Training setup completed:\")\n",
    "print(f\"Loss function: Hybrid (BCE + InfoNCE)\")\n",
    "print(f\"Optimizer: AdamW (lr=1e-4, weight_decay=1e-5)\")\n",
    "print(f\"Scheduler: ReduceLROnPlateau\")\n",
    "print(f\"Device: {device}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
